<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Illusions in the Boardroom by Adrian McPhee — Preview</title>
  <meta name="description" content="AI can now read and reconcile what your organisation says with what your systems do. Read the opening chapters of AI Illusions in the Boardroom by Adrian McPhee.">
  <meta name="author" content="Adrian McPhee">
  <meta property="og:type" content="book">
  <meta property="og:title" content="AI Illusions in the Boardroom by Adrian McPhee">
  <meta property="og:description" content="AI can now read and reconcile what your organisation says with what your systems do. Read the opening chapters free.">
  <meta property="og:image" content="https://simpleisadvanced.com/ai-illusions-in-the-boardroom/og-image.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <link rel="canonical" href="https://simpleisadvanced.com/ai-illusions-in-the-boardroom/">
  <style>
    body { max-width: 42em; margin: 2em auto; padding: 0 1.5em; font-family: Georgia, serif;
           line-height: 1.7; color: #222; background: #fff; }
    h1 { font-family: system-ui, sans-serif; font-size: 1.8em; margin-bottom: 0.2em; }
    h1 + p { color: #666; font-style: italic; margin-top: 0; }
    h2 { font-family: system-ui, sans-serif; font-size: 1.3em; margin-top: 2.5em;
          border-top: 1px solid #e5e5e5; padding-top: 1.5em; }
    h2.part { font-size: 0.85em; text-transform: uppercase; letter-spacing: 0.1em;
              color: #999; border: none; margin-bottom: -1em; padding-top: 2em; }
    h2 a { color: inherit; text-decoration: none; }
    h2 a:hover { color: #c0392b; }
    h3 { font-family: system-ui, sans-serif; font-size: 1.05em; margin-top: 1.5em; }
    p { margin: 0.8em 0; }
    .continue { font-family: system-ui, sans-serif; font-size: 0.9em; }
    .continue a { color: #c0392b; text-decoration: none; }
    .continue a:hover { text-decoration: underline; }
    .header { text-align: center; margin-bottom: 3em; }
    .header img { max-width: 200px; margin-bottom: 1em; }
    .cta { text-align: center; margin: 3em 0; padding: 2em; background: #f9f9f9;
            border-radius: 8px; font-family: system-ui, sans-serif; }
    .cta a { display: inline-block; padding: 0.7em 2em; background: #222; color: #fff;
             text-decoration: none; border-radius: 4px; font-weight: 600; }
    .cta a:hover { background: #444; }
    .banner { position: sticky; top: 0; z-index: 100; background: #222; text-align: center;
               padding: 0.75em 1em; font-family: system-ui, sans-serif; font-size: 0.9em;
               margin: -2em -1.5em 2em; }
    .banner a { color: #fff; text-decoration: none; font-weight: 500; }
    .banner a:hover { text-decoration: underline; }
    footer { text-align: center; color: #999; font-size: 0.85em; margin-top: 3em;
             padding-top: 1.5em; border-top: 1px solid #e5e5e5; }
  </style>
</head>
<body>
  <div class="banner">
    <a href="./">Read in the interactive reader &rarr;</a>
  </div>

  <div class="header">
    <img src="cover.png" alt="AI Illusions in the Boardroom">
    <h1>AI Illusions in the Boardroom</h1>
    <p>What happens when machines can read what you say alongside what your systems do?</p>
    <p style="font-style: normal; font-size: 0.9em;">Adrian McPhee</p>
  </div>

<section id="ch-0">
<h2><a href="./?ch=0">Introduction</a></h2>
<p>By now, it is clear that AI can generate content. It drafts strategy documents, summarises board packs, analyses contracts, explores financial models, generates production code. Most executives use it daily. This book is not about that capability.</p>
<p>It is about the ability of AI to read and reconcile, and the tremendous effect this is set to have on competition, operating models, and the people working inside them.</p>
<p>When a machine can read what you say alongside what your systems do, the gap between narrative and reality becomes measurable. A strategy document can be compared against the code that must implement it. An architecture reference model can be compared against what is actually running. A budget line can be compared against the operational outcomes it was meant to produce. A portfolio label can be compared against the services that supposedly comprise it. An investment case can be compared against the incident costs it was meant to prevent.</p>
<p>These comparisons are not hypothetical. They are available today, at negligible cost, to any organisation willing to point an AI system at its own artefacts and ask a simple question: does what we say match what we do?</p>
<p>Once these artefacts can be read together, the organisation’s claims become testable. Structural honesty becomes measurable rather than aspirational.</p>
<p>For most organisations, the answer is no. Not because the people inside them are dishonest, but because the structures they operate within have made honesty expensive and vagueness cheap for more than a decade.</p>
<p>This book uses a specific term: software-dependent corporate. It refers to large, established organisations whose products and operations depend critically on software, but whose governance structures were designed for a world in which software was a support function.</p>
<p>Banks, insurers, retailers, logistics firms, airlines, public sector bodies, and older companies that describe themselves as technology companies but still operate with inherited governance assumptions all fall into this category. These organisations are not pre-software. They have had software for decades. What they have not adapted to is the shift that occurred between roughly 2008 and 2012, when software stopped being infrastructure and became the primary medium through which these companies create and deliver value.</p>
<p>By 2012, the structural change was complete: these organisations needed to become software companies. They hired engineers by the hundreds, adopted continuous delivery, and launched digital transformation programmes. The governance structures, the budget cycles, the HR frameworks, and the executive mental models stayed where they were. The technology changed. The power structures did not. The misalignment has been compounding ever since.</p>
<p>The most reliable diagnostic is linguistic. If your organisation talks about “the business” as a group of people who are not engineers, your organisation is a software-dependent corporate. If engineers are considered to work in the business by default, it probably is not. Once that split exists, the gap between what the organisation says and what the system does becomes self-sustaining.</p>
<p>Software-dependent corporates evolved structures that separate “the business” from engineering, producing representations with no operational counterpart: narratives that replace reality, portfolio labels with no executable boundary, avoidant architecture, processes that were never written down. These are not failures of intelligence. They are coping mechanisms. They are how intelligent people remain functional inside structures that will not let them do real work. I have watched this pattern repeat in every large organisation I have worked in, across fintech, retail, and mobility, and the consistency is itself diagnostic.</p>
<p>For two decades, these organisations absorbed structural deficits socially: through meetings, translation roles, governance ritual, and institutional memory. In every one of them, someone carried the real system in her head: she knew which architecture diagrams were fiction, which products did not exist as coherent systems, which processes survived only as institutional memory. She spent her weeks translating between functions, reconciling artefacts that the organisation’s structure had separated and no one had authority to rejoin, building nothing, deciding nothing, holding the organisation together. It was inefficient, fragile, and entirely dependent on the continued presence and goodwill of people like her. It held, until AI made the same synthesis available at negligible cost.</p>
<p>There is a simple test for any software-dependent corporate. If an AI system reads your strategy documents, your architecture models, your process descriptions, and your code together, will the synthesis converge on something coherent? Or will it collapse into contradiction?</p>
<p>For most organisations, it collapses. The strategy describes interfaces the code does not implement. The architecture reference model describes boundaries the system does not observe. The portfolio contains products that no engineer can map to a coherent set of services. The investment case omits costs the incident log makes visible. Each gap has been survivable individually for years. What changes is the cost of knowing. Reconciliation that once required weeks of consulting time and political capital is now available continuously, cheaply, and without political filtering.</p>
<p>Cheap synthesis does not force truth. It forces a choice: pay the rising cost of suppression, or authorise the structural changes that make truth useful. This book is about that choice.</p>
<p>Part I, When Machines Read, introduces the inversion. What changes is not what AI can produce, but what it can reveal.</p>
<p>Part II, The Illusion Was Rational, shows how software-dependent corporates produce and stabilise representations that do not correspond to the systems they describe. The illusions are not failures of intelligence. They are coping mechanisms with a cost structure.</p>
<p>Part III, Capital Mispricing and Structural Risk, traces the consequences. When the gap becomes measurable, capital allocation, coordination costs, talent retention, and regulatory exposure are all repriced.</p>
<p>Part IV, Structural Correction, describes the specific structural commitments that make an organisation machine-readable: process ownership, contracts, data as published product, and the executive decisions that authorise them.</p>
<p>The conclusion presents the choice: accelerate narrative production, or reconcile reality continuously. The two paths are incompatible without structural change.</p>
<p>This book is written for the executive team: the CEO, the board, the CFO, the CHRO. The technical specificity (microservices, architecture, data contracts) is retained because the board cannot authorise structural changes it does not understand. The level of detail is what the board needs to see in order to know what it is approving.</p>
<p>The vignettes throughout this book are illustrative. They are composited and fictionalised to represent structural patterns the author has observed across consulting engagements in financial services, retail, logistics, and media. Details, including industry, geography, headcount, individual roles, and financial figures, have been invented or altered. No vignette describes a specific organisation. The patterns are real. The examples are not.</p>
</section>
<section id="ch-1">
<h2><a href="./?ch=1">Executive Summary</a></h2>
<p>Thesis. Software-dependent corporates evolved structures that separate “the business” from engineering, producing representations with no operational counterpart: narratives that replace reality, portfolio labels with no executable boundary, architecture that advises but does not bind, processes that were never written down. These are not failures of intelligence. They are coping mechanisms whose cost structure has changed. AI makes reconciliation cheap and continuous. The gap between what the organisation says and what the system does becomes measurable. Each quarter of informed inaction extends a record the board will eventually be asked to explain.</p>
<p>Diagnosis. The illusions are structural, not personal. Products exist in portfolios but have no coherent boundary in the code. Architecture describes aspirations rather than systems. Capital attaches to labels while value is created at process boundaries that the financial model cannot observe. Coordination consumes 30--40\</p>
<p>Six demands. Before any AI investment, the board should require management to produce six items for a single critical process: (1) a process inventory ranked by revenue contribution and risk exposure, with a named owner for each, (2) a structured process definition with states, transitions, decision points, and failure modes, (3) a single named owner with a charter and boundary, not a committee, (4) a contract inventory listing the units this process depends on, with versioned, testable interfaces, (5) a reconciliation report comparing the definition against the code, and (6) a cost-to-outcome trace connecting investment to operational performance. If management cannot produce these for one critical process within 30 days, every AI investment builds on a foundation the organisation has never inspected.</p>
<p class="continue"><a href="./?ch=1">Continue reading →</a></p>
</section>
<h2 class="part">When Machines Read</h2>
<section id="ch-2">
<h2><a href="./?ch=2">Chapter 1: AI as the Unforgiving Reader</a></h2>
<p>A large European telecoms operator decides to pilot an AI assistant for its technology leadership. The brief is modest: given access to the company’s strategy documents, architecture diagrams, and code, the assistant should produce a summary of the organisation’s technology landscape and its alignment with stated strategic priorities.</p>
<p>The CTO is supportive. The experiment is sanctioned. A small team feeds the assistant the relevant artefacts: the annual strategy deck, the technology vision document, the architecture reference model (last updated fourteen months ago), the current service catalogue, and read access to the primary code repositories.</p>
<p>The assistant produces a twelve-page synthesis that is calm, precise, and devastating.</p>
<p>It notes that the strategy deck identifies “digital self-service for enterprise customers” as a top-three priority, but that only two of forty-seven services have any self-service provisioning logic, and both are experimental prototypes with no production traffic. It observes that the architecture reference model describes eight distinct areas of business responsibility, each meant to own its own data, but that the actual system follows a different structure: one organised around legacy databases rather than business domains. It identifies fourteen services that share a single database, contradicting the reference model’s assertion of independent ownership.</p>
<p>The summary does not editorialise. It reads what the organisation claims alongside what the code shows, and lists the contradictions.</p>
<p>The CTO reads the summary. He recognises every finding. None of it is new to him. What is new is the vertigo of seeing it confirmed in someone else’s words, after years of carrying it alone. But it is written down in a single document, produced in hours rather than months, and cannot be dismissed as one person’s opinion.</p>
<p>He forwards the summary to the CEO. The CEO’s response arrives within a day. She does not dispute the findings. She asks that the summary not be shared further, because “the board narrative is different, and we are not ready to reconcile.”</p>
<p>The CTO proposes a compromise: reconcile one business domain, network provisioning, as a limited pilot. The CEO declines. The pilot would produce artefacts that contradict the strategy deck submitted to the board three weeks from now. The objection is not that reconciliation is premature. It is that reconciliation would produce evidence.</p>
<p>The summary is saved in a folder with restricted access. The AI pilot is described as “promising but premature.” The strategy deck is presented to the board the following month, unchanged. It describes an organisation that the synthesis showed does not exist. The CTO now carries two things he did not carry before: confirmation that his reading of the system was correct, and confirmation that the organisation prefers not to know.</p>
<p>The CEO’s decision was not to defer reconciliation but to prevent it. That distinction matters. The decision preserved narrative coherence at the expense of reconciled evidence.</p>
<p>Artificial intelligence is most commonly discussed as a creator: writing code, drafting documents, generating answers, producing images. Most enterprise AI investment targets this capability. The rationale is understandable: these are real productivity gains. They are also the less important use of the technology.</p>
<p>AI’s real power in organisations is not that it produces artefacts but that it reads them. It reads without fatigue, without respect for hierarchy, narrative confidence, or institutional memory. It does not know who wrote a document or how many people approved it. It only cares whether what exists corresponds.</p>
<p>Most organisations get this distinction backwards. The generator accelerates the part that was never the bottleneck. Writing code was never the hard part of building software. Design is: deciding what to build, where the responsibilities lie, how the pieces interact, and what happens when they fail. These decisions depend on clear business processes, explicit ownership, and precise descriptions of how value is created. No code assistant helps with this, because the inputs are not code. They are organisational structure, business logic, and customer journeys, and in most software-dependent corporates, these inputs do not exist in a form that any tool, human or machine, can reason about reliably.</p>
<p>The generator also reinforces a comfortable assumption: that AI is a tool that works for people inside the existing organisational structure, enhancing their existing capabilities. This does not threaten anyone’s role. It does not challenge any power structure. The reader does both. It helps organisations see themselves clearly, which is why most organisations deploy the generator first and the reader never.</p>
<p>When AI is used as a reader, as a system that continuously reconciles what the organisation says with what it does, the implications are structural. The reader does not produce new content. It produces accountability. It takes artefacts that were designed to exist independently and reads them together, surfacing contradictions that the organisation has been maintaining socially.</p>
<p>Consider the telecoms vignette. The strategy deck, the architecture model, the service catalogue, and the code had all existed for years. They had been reviewed by different people at different times for different purposes. No one had read them together and asked: are these consistent? The AI did not discover anything the CTO did not already know. It enumerated what he knew in a form that could not be dismissed as opinion or interpreted away through social negotiation.</p>
<p>The reader does this across multiple domains simultaneously.</p>
<p>Strategy versus code. The AI parses the strategy for testable claims. “API-first design” is a testable claim. “Drive digital transformation” is not. For each testable claim, it examines the code for evidence. The output is not a judgement. It is an inventory: 47 services, 312 connections between them, of which 119 (38\</p>
<p>The AI gets things wrong, and the kinds of errors are instructive. It inflates the service count by treating internal components as separate systems. It counts monitoring traffic as real communication, which is technically correct and analytically useless. It cannot distinguish between a workaround that persists because no one has fixed it and a design choice that persists because someone decided it was correct. Context, intent, and history are invisible to the reader. Structure is not.</p>
<p>These are not edge cases. They are the permanent boundary of what machine reading can do. A routing rule that bypasses the defined process may be a bug, a workaround that prevents a worse failure, or a design choice that was never documented. The AI cannot tell the difference. The human triage step is not a temporary cost of early adoption. It is permanent. Organisations that expect the machine to replace judgement will produce reconciliation reports that are precise, comprehensive, and wrong in exactly the ways that matter.</p>
<p>The practical ceiling is worth describing honestly, because the sceptical CTO who dismisses reconciliation as immature is making a different error from the enthusiast who expects it to replace judgement. The first reconciliation cycle produces hundreds of findings. Most are technically correct and analytically useless: naming inconsistencies, stale monitoring endpoints, artefacts that exist in the repository but carry no production traffic. The signal-to-noise ratio is poor because the reader reports the obvious alongside the significant with equal confidence.</p>
<p>Reconciliation genuinely falls apart in three conditions. First, processes that span organisational boundaries where artefacts are maintained by different teams in different formats: the reader can compare what it is given, but it cannot reconcile a Confluence page against a Swagger specification against a spreadsheet labelled “assumptions.” Second, orchestration that was never documented because it evolved through informal coordination: the process exists in people’s behaviour, not in any artefact, and the reader has nothing to read. Third, the intent-implementation distinction described above, which is permanent.</p>
<p>The triage workload follows a decay curve. The first cycle requires a domain expert spending two to three days reviewing the output, separating genuine divergence from noise. The second cycle halves the review, because the noise categories are now classified. By the fourth cycle, the expert reviews exceptions rather than the full output, and the reconciliation functions as a continuous audit rather than a periodic exercise. The tool improves not because the AI becomes smarter but because the organisation produces better artefacts for it to read.</p>
<p>There is a subtler failure mode. Once reconciliation is continuous, people will learn to write artefacts that pass the audit rather than artefacts that describe the truth. Process definitions will converge on what the code does rather than what the process should do. Formal compliance will replace substantive accuracy. The tool audits consistency, not quality. Quality still requires ownership.</p>
<p>Process definition versus system behaviour. When an organisation has explicit process definitions and code, AI can compare the two. Take a claims process: “when a claim is submitted, the system validates required fields, assigns a claim number, and routes to the appropriate assessor queue based on claim type and value.” The AI can verify whether the code actually does this.</p>
<p>The results are often revealing. The process definition says claims are routed by type and value. The code shows that routing also depends on an undocumented field set by a legacy system. The process definition says validation occurs at submission. The code shows that some fields are validated after the fact, and failures are logged but not surfaced to the submitter. The process definition does not mention a retry mechanism. The code has one, and it has been silently masking data quality issues for eighteen months. These are the kinds of discoveries that organisations make when they first subject their stated processes to machine reading.</p>
<p>Architecture diagrams versus what is actually running. The AI reads an architecture reference model and compares it to the systems that are actually deployed. The discrepancies are always present: shared databases where the diagram shows independence, direct calls between systems where it shows clean separation, services organised around technical convenience where it describes business domains.</p>
<p>Consider a claims management process at a mid-sized European insurer. The team responsible has described its process in structured terms: eight states a claim can occupy, the transitions between them, the triggers, and the failure modes. Every state, every transition, and every exception is explicit and versioned.</p>
<p>The AI reader consumes this definition alongside the code that implements the claims process. It produces a reconciliation report:</p>
<p>Reconciliation: claims management process v1.2 against the running system</p>
<p>Divergent (definition does not match implementation):</p>
<p>The timeout for requesting documents is defined as five business days. The system counts calendar days, not business days. Claims submitted on Wednesday that require documents will timeout on Monday (five calendar days) rather than the following Wednesday (five business days). Impact: premature escalation of approximately 15\</p>
<p>Undefined (implemented but absent from definition):</p>
<p>A re-review step exists in the system: when an assessor sends a claim back for further review, it returns to the under review state. This step is not in the process definition. It is used approximately 40 times per month. Without it, re-reviewed claims would need to be declined and resubmitted.</p>
<p>Unimplemented (defined but not found in code):</p>
<p>The escalation for overdue documents is defined in the process, but the system silently exempts claims valued below € 500. This exemption exists nowhere in the process definition.</p>
<p>The reconciliation took eleven minutes. A consulting engagement would take two to four weeks and would be unlikely to identify the business-day versus calendar-day discrepancy, because that discrepancy lives at the intersection of a process rule, a scheduling configuration, and a date calculation: three artefacts that no single interviewee would hold in mind simultaneously.</p>
<p>The reconciliation is only possible because the process definition exists as a structured, machine-readable artefact. Without it, the AI reader can describe what the code does, but it cannot identify where the code diverges from intent, because intent was never written down. By the third or fourth weekly cycle the useful findings far outweigh the noise, and the reconciliation functions as a continuous audit of whether the organisation’s processes behave as described.</p>
<p>The reader’s power is not analytical insight. It is the production of accountability by enumerating contradictions.</p>
<p>In organisations where reality has been mediated through layers of representation, humans perform an enormous amount of interpretive labour. They bridge gaps between strategy and execution with context, intuition, and social negotiation. They absorb contradictions implicitly. They carry exceptions in their heads. They know that the architecture diagram is wrong but act as though it is right, because acting otherwise would require a conversation nobody wants to have.</p>
<p>This labour is invisible and unpaid. It is also what allows illusions of work to persist. When the organisation’s descriptions are too vague for the AI to reason about, it fills gaps with inference, not because the technology is immature but because the inputs are empty. The parallel between human and machine interpretation is instructive. Both have the same root cause: insufficient precise information. The difference is that human gap-filling is invisible, hiding inside institutional knowledge and social negotiation, while AI gap-filling is visible and embarrassing. This visibility is the reader’s gift to the organisation. It makes the cost of vagueness undeniable.</p>
<p>When the AI produces a list of contradictions, the organisation can no longer claim ignorance. The contradictions are documented, enumerated, and attributed. Someone must respond.</p>
<p>The response is revealing. The CEO in the telecoms vignette did not dispute the findings. She suppressed them. She asked that the summary not be shared further. This is rational behaviour. The board narrative was different, and closing the gap would have required confronting structural problems that the organisation had been absorbing socially for years.</p>
<p>There is a subtler response: constraining the reader rather than confronting it. AI access is limited to sanitised artefacts. Outputs are reviewed before circulation. The synthesis is filtered through the same narrative layer it was meant to bypass. Vendor incentives reinforce this: enterprise AI products are designed to complement existing workflows, not to challenge them. Regulatory caution provides further cover. Data governance, model risk, and compliance concerns are real, but they are also convenient reasons to keep the auditor on a short leash. The technology is neutral. Its deployment never is.</p>
<p>The suppression response is rational but not stable. It can be sustained when the cost of reconciliation exceeds the cost of suppression. Cheap synthesis changes the ratio.</p>
<p>When synthesis required a twelve-week consulting engagement, suppression was easy. The engagement happened once, produced a report, and the report could be filed. The gap between engagements allowed the organisation to absorb the findings without changing anything.</p>
<p>When synthesis is continuous, suppression becomes expensive. The AI reader produces a new synthesis every week. Each cycle surfaces the same contradictions, plus any new ones that have accumulated. The organisation must actively manage the output: filtering it, restricting access, explaining why findings are not being acted on. The effort required to maintain the illusion rises while the cost of replacing it with clarity falls.</p>
<p>The dynamic is already observable, driven by the reduced cost of verification.</p>
<p>For boards, the implication is direct. When synthesis is expensive, the board has no alternative to governing through unverified representations. When it is cheap, the board has a choice. It can demand artefacts that have been reconciled against the system before they arrive at the boardroom. It can require that the contradictions are surfaced, not smoothed. Whether the board makes this demand depends on whether it prefers comfort or clarity.</p>
<p>The constraint that the reader surfaces is structural, not technical. The technology is available today. The question is whether the organisation is willing to let it read.</p>
</section>
<section id="ch-3">
<h2><a href="./?ch=3">Chapter 2: When AI Writes the Illusion</a></h2>
<p>The quarterly board pack is due in three days. The strategy team has been preparing it for two weeks, but the technology section is still weak. The CTO’s team has submitted their update: accurate, detailed, and unreadable. Twelve pages of service dependencies, incident summaries, and migration status. The board will not engage with it.</p>
<p>So the chief of staff feeds the CTO’s update into an AI assistant, along with the company’s strategy document and last quarter’s board narrative. The prompt is simple: produce a two-page technology summary that connects the current state of the technology estate to the company’s strategic priorities, suitable for a board audience.</p>
<p>The result is impressive. The summary connects the migration of three services to a strategic priority around customer self-service. It cites a 23\</p>
<p>The CTO reads it carefully.</p>
<p>The 23\</p>
<p>Every fact in the summary is individually defensible. The synthesis is internally coherent but externally unverified.</p>
<p>The CTO marks up three corrections, sends it back, and books a meeting to review them before the board deadline, knowing it will not survive the week. It is rescheduled once, then dropped. The board reads the uncorrected summary. The CEO notes that the technology narrative “finally feels aligned with strategy.” Nobody asks whether the alignment is real.</p>
<p>The previous chapter described AI as an unforgiving reader. This chapter describes its counterpart: AI as a fluent writer of organisational narrative.</p>
<p>AI-generated narrative is dangerous not because it is false but because it is plausible.</p>
<p>A human writing a strategy update must work from what she knows. Her knowledge is incomplete, her time is limited, and her prose reflects both. The gaps are visible. The vagueness is detectable. A reader with system knowledge can see where the narrative departs from reality, because the departure is clumsy.</p>
<p>An AI writing the same update works from whatever it is given. If the inputs include a strategy document and a set of metrics, the AI will find connections between them, even if the connections are spurious. It will use architectural vocabulary correctly. It will cite numbers accurately. It will produce a synthesis that reads as though someone with deep understanding wrote it.</p>
<p>The AI does not have deep understanding. It has inputs. If the inputs are disconnected from reality, the output will be disconnected from reality. The difference is that the output will not look disconnected. It will look coherent, authoritative, and aligned.</p>
<p>AI-generated narrative removes the friction that once made illusion detectable. The person who prompts the AI may not know whether the output is grounded. The executive who reads it almost certainly does not. The narrative becomes self-reinforcing: it sounds precise, therefore it is treated as precise, therefore nobody checks whether it corresponds to anything real. The organisation has not become clearer. It has become more confidently wrong.</p>
<p>AI makes narrative cheap, which deepens the problem.</p>
<p>In the old model, producing organisational narrative required effort. A strategy deck took weeks. A board pack consumed days. The expense acted as a natural constraint. The effort of production forced at least some engagement with the underlying reality.</p>
<p>AI removes this constraint. Narrative becomes abundant. Strategy updates can be generated weekly instead of quarterly. Status reports can be produced for every team rather than every division. Product documents can be drafted for every idea rather than every commitment. The organisation generates more narrative, more frequently, with less human effort.</p>
<p>The adversarial case, that AI generates better illusion, appears to undermine the argument of this book. If AI makes pretence more convincing, how can AI also be the force that destabilises it?</p>
<p>The answer lies in an asymmetry between generation and reading.</p>
<p>AI-as-generator operates on narrative inputs. It reads documents, metrics, and descriptions. It produces a synthesis of what those documents say. It cannot check whether they are true. It has no independent access to the system. It works with representations, and if the representations are wrong, the output is wrong. Fluently, convincingly, dangerously wrong.</p>
<p>AI-as-reader operates on artefacts that bind. It reads the code. It reads what is actually deployed and running. It reads the process definitions wired into the running system. It cannot be fooled by better prose, because it is not reading the prose. It is reading the system.</p>
<p>The asymmetry is decisive. The generator can produce a board summary claiming the platform migration is 70\</p>
<p>No amount of polished narrative survives contact with a system reader. The better the narrative, the more embarrassing the contradiction when the reader surfaces it.</p>
<p>There is a predictable misreading here. The solution is not better documentation with an AI layer on top. The reader only has force when the artefacts it reads are binding, when deviation carries consequence and someone is accountable for resolving it. Without structural ownership, AI does not expose illusion. It industrialises it.</p>
<p>Every software-dependent corporate now faces a fork between these two uses.</p>
<p>One path uses AI to write. To generate better narratives, smoother updates, more convincing decks. This path is easy, immediately rewarding, and politically safe. It threatens no power structure. It challenges no assumption. It makes the existing model more efficient.</p>
<p>The writer path is not merely easier. It is corrosive. An organisation that deploys generative AI without structural reconciliation does not merely fail to improve. It increases its regulatory exposure: faster narrative production generates more discoverable artefacts that a regulator, an acquirer, or a litigator can compare against the system. The volume of unverified claims grows while the evidence base against them remains unchanged. Each quarter on it calibrates expectations to the generated output. Planning velocity increases while execution capacity stalls. The narrative becomes more fluent as the foundation beneath it degrades, making the gap harder to detect and the structural work harder to motivate. The symptoms that should have triggered correction are narrated away before they reach anyone with authority to act.</p>
<p>The other path uses AI to read. To compare strategy with systems, process definitions with code, cost allocation with value creation. This path is uncomfortable, politically dangerous, and structurally transformative. It threatens everyone whose authority depends on controlling the narrative rather than understanding the system, and in doing so reveals which organisations are governed through evidence and which are governed through prose.</p>
<p>Most organisations will try to do both. They will discover that the two uses are not complementary. They are contradictory.</p>
<p>Consider what happens in practice. A product team uses an AI assistant to draft a product requirements document for a new self-service claims feature. The PRD is fluent and specific: it describes user flows, defines acceptance criteria, and references the existing claims architecture by name. It takes thirty minutes to produce. The team is pleased. The PRD is circulated.</p>
<p>The following week, an engineering lead runs the organisation’s AI synthesis against the code alongside the new document. The synthesis flags three problems. The “existing claims architecture” the PRD references was decommissioned six months ago; the current implementation uses a different service with different data contracts. Two of the described user flows require access to a customer record service that the claims team does not own and has no contract with. The acceptance criteria reference a response time SLA that no system in the current architecture can meet.</p>
<p>The document sounded precise. The system is not. The AI wrote a convincing description of something that cannot be built as described, and the AI reader caught it in minutes.</p>
<p>The organisation now has a choice: fix the document to match reality, or suppress the synthesis and ship the document as written. The first is uncomfortable. The second is expensive. Both reveal that the two uses of AI, writing and reading, pull in opposite directions unless the writing is grounded in the same reality the reader inspects.</p>
<p>The fork is a governance choice, not a technology choice. It asks whether you want AI to help you describe reality or to help you avoid it. The economics of each path have already diverged.</p>
<p>The CTO of a mid-sized European insurer commissions a quarterly consulting engagement to reconcile the technology estate against the strategy. The engagement takes eight weeks and costs € 180,000. The consultants produce an accurate report. Before the findings reach the board, the CTO reviews them and softens the language. Three months later, the same gaps persist. The next engagement produces the same findings.</p>
<p>The CTO observes that the cost of knowing has remained constant while the gap has widened. She runs the same analysis using AI synthesis. It takes two days. The output is comparable. She runs it again the following week. The cost of knowing has collapsed. The cost of not knowing has not changed. The asymmetry has reversed.</p>
<p>Software-dependent corporates evolved entire functions, programme management, portfolio governance, strategy offices, transformation teams, whose purpose was to maintain coherence at the narrative level without requiring coherence at the system level. AI collapses the cost of the synthesis these functions perform. The shift is from lagging to leading: contradictions surface when they are introduced, not when they cause damage.</p>
<p>Cheap synthesis without ownership produces insight without consequence.</p>
<p>The constraint is economic, not technological. The tools exist. The remaining obstacle is not capability but willingness, and the willingness requires structural change, which is the subject of the rest of this book.</p>
</section>
<h2 class="part">The Illusion Was Rational</h2>
<section id="ch-4">
<h2><a href="./?ch=4">Chapter 3: The Board Narrative and the System</a></h2>
<p>The CEO of a mid-sized European insurer, approximately 4,000 employees with a core policy administration platform dating from 2007, is eighteen months into her tenure. She has done what a competent CEO should do. She hired a respected CTO from a digital-native company. She funded a three-year technology modernisation programme at € 40 million, more than the board wanted, less than the CTO asked for, a reasonable compromise. She approved a new organisational structure: product-aligned teams, a platform group, an architecture function. She attended the first two quarterly reviews personally, to signal commitment.</p>
<p>The strategy is coherent. The CTO is capable. The funding is real. The board is supportive.</p>
<p>Two years in, the core measures she cares about have not moved: time to market for new products, operational cost per policy, customer retention. Two major initiatives have been quietly descoped. The CTO reports progress. The CFO reports cost. Neither report contradicts the other. Neither explains why the organisation feels no different.</p>
<p>She asks her direct reports. Each gives a plausible account. The CTO says the legacy systems are harder to untangle than expected. The CFO says the technology spend is growing faster than the revenue it was meant to unlock. The COO says the teams are still learning to work in the new model. None of them is lying. None of them is wrong. None of them is describing the same organisation.</p>
<p>She commissions an external review. The consultants produce 120 slides. The diagnosis is familiar: alignment gaps, unclear ownership, siloed delivery. The recommendation is another reorganisation, a new operating model, a governance framework. She has seen this pattern before, at her previous company. She approved the same kind of review there. It produced the same kind of recommendations. She left before the results were visible. This time she stays. The results will not be visible this time either.</p>
<p>She suspects the problem is not alignment, or talent, or even legacy technology. She suspects it is something in the structure itself, something that makes every reasonable decision produce unreasonable outcomes. She cannot name it, because nothing in her reporting, her reviews, or her advisory relationships is designed to make it visible.</p>
<p>She approves the consultants’ recommendations. There is nothing else to approve.</p>
<p>This is a failure of legibility: no business process in the organisation is defined explicitly enough to connect the strategy to the systems that must enact it. Each report is locally rational. None can be reconciled with the others, because no shared description of how the organisation actually works exists to test them against.</p>
<p>The CEO is not incompetent. She is governing through representations that cannot be reconciled. This was the rational outcome of a governance model designed before software became the medium through which the organisation creates value. The representations she receives were not designed to be tested against the system. They were designed to be approved.</p>
<p>Every board of a software-dependent corporate receives a narrative. It arrives as a board pack, a set of management reports, a quarterly business review. It contains financial performance, programme status, risk registers, and strategic progress. Each section is produced by a different function. Each section is internally consistent. Each section describes the organisation from its own perspective.</p>
<p>The board reads these sections as a composite picture of the organisation. They are not. They are parallel descriptions of different organisations, each using its own language, its own measures, and its own definition of success.</p>
<p>The CFO’s section reports financial performance against budget. It measures cost, revenue, and margin. It tracks capital expenditure on technology programmes. It is accurate, detailed, and complete within its own frame. What it does not do is connect expenditure to system behaviour. A € 40 million technology programme appears as a cost line. Whether the programme has changed anything in the system that creates value is not measured, because the financial model was not designed to connect cost to system capability.</p>
<p>The CTO’s section reports technology progress. Deployment frequency is up. A new customer-facing application has launched. The recruitment brand is stronger. These are real achievements. What the section does not do is connect them to the board’s strategic priorities. “Deployment frequency” measures engineering activity. It does not measure whether the organisation can bring a new product to market faster. The new application launched, but the processes it supports still span six services maintained by three different teams. The application is a surface. The structure underneath it has not changed.</p>
<p>The COO’s section reports operational performance. Claims processing times, customer satisfaction scores, incident frequency. Each metric is defined, tracked, and reported with rigour. What the section does not do is explain whether operational performance is constrained by the system or by the people running it. A claims processing time of 14 days may reflect a complex process or a system that routes claims through five services that share no common data model. The metric does not distinguish between the two. The board cannot tell.</p>
<p>Each section is locally consistent. Taken together, they are globally incoherent. The incoherence persists because no mechanism exists to reconcile these representations against the system that implements them.</p>
<p>This incoherence is not new. What is new is naming the mechanism that produces it.</p>
<p>These organisations govern through representations: strategy documents, operating models, architecture diagrams, portfolio views, KPI dashboards, board packs. These representations serve a legitimate purpose. No CEO or board member can understand every system in a large organisation. Representations provide the abstraction necessary for governance at scale.</p>
<p>The problem arises when representations replace the thing they represent.</p>
<p>A portfolio view that shows “Customer Engagement” as a product with a P\&amp;L and a roadmap is a representation of a business capability. If the portfolio treats it as something that can be funded, measured, and held accountable, it has confused the label with the system. In many organisations, the product called “Customer Engagement” has no coherent boundary in the code. It is a name applied to a collection of features scattered across services maintained by different teams. The portfolio label exists. The product does not.</p>
<p>This pattern has a specific structure. The representation is produced by someone who understands one part of the system. It is reviewed by someone who understands a different part. It is approved by someone who understands neither but needs a basis for decision. At each stage, the representation becomes more polished and less connected to reality. By the time it reaches the board, it is locally consistent, professionally formatted, and describes an organisation that does not exist.</p>
<p>How does this incoherence survive? Why does the board not notice?</p>
<p>The answer is absorption. The gap between the board narrative and the system is bridged socially, by people whose job is to translate between the two. These translation roles are so pervasive that they are invisible. Programme managers, delivery leads, portfolio analysts, business partners: each exists to bridge representations that cannot be reconciled structurally. They are not performing unnecessary work. Without them, the gap between narrative and reality would produce immediate, visible failure. But the synthesis they provide is expensive, fragile, and entirely dependent on the continued presence of the people performing it.</p>
<p>The absorption mechanism creates a specific failure mode. Because the translation is social rather than structural, it cannot be audited, versioned, or transferred. When a programme manager who has spent three years bridging between the CTO’s technology architecture and the CFO’s cost model leaves the organisation, that bridge collapses. The knowledge was never written down in a form that someone else could use. It existed in the person’s head, in the relationships they had built, in the mental model they had constructed by attending hundreds of meetings.</p>
<p>The organisation responds by hiring a replacement and waiting six to twelve months for the new person to reconstruct the bridge. During this period, the gap between narrative and system widens, decisions are made with less accurate information, and costs accumulate invisibly.</p>
<p>Boards were designed to govern through representations. Financial reports, management accounts, risk registers, and strategic plans are all representations. This is appropriate and necessary. A board cannot and should not understand every service, every data flow, every process exception in a large organisation.</p>
<p>The question is what happens when the representations cannot be tested against the system they describe.</p>
<p>When the representations are accurate, governance works. When they are disconnected from reality, no mechanism exists to test whether the decisions they inform add up. The board approves a € 40 million programme not knowing that the system cannot deliver on the business case without structural changes the programme does not include. It measures progress by deployment frequency, not knowing that the deployments are unconnected to strategic priorities.</p>
<p>Boards are not incompetent. They are governing through artefacts that reach them only after the structural contradictions have been smoothed away.</p>
<p>A board pack that presents financial performance, technology progress, and operational metrics as three separate sections is three parallel descriptions rather than a synthesis. The synthesis either happens in the room, carried in executive memory, or it does not happen at all.</p>
<p>When AI makes reconciliation cheap, this constraint changes. The board does not need to understand every service. It needs artefacts that have been verified against the system before they arrive, so that the contradictions are surfaced, not smoothed. The question is whether the organisation is willing to produce those artefacts. That willingness requires structural change, which is the subject of the rest of this book.</p>
</section>
<section id="ch-5">
<h2><a href="./?ch=5">Chapter 4: Portfolio Labels That Do Not Exist in Code</a></h2>
<p>The product review is held monthly. Each product owner presents to the CPO and a panel of senior stakeholders. The product owner for “Customer Engagement” presents third. His slides show a funnel, a set of KPIs, and a roadmap with six items colour-coded by quarter. The presentation is fluent, the narrative confident. Customer Engagement is growing. Retention is improving. The team has delivered twelve features this quarter.</p>
<p>Then an enterprise architect asks a question. “Which services comprise Customer Engagement?”</p>
<p>The room shifts. The product owner begins to answer, then pauses. Customer Engagement, it turns out, is not a system. It is a label applied to a collection of features that span four different services, maintained by three different teams, two of which report into a different product area. The notification service is shared with Operations. The recommendation engine sits inside a legacy monolith nominally owned by Transactions. The analytics pipeline runs on infrastructure maintained by a platform team that was recently reorganised.</p>
<p class="continue"><a href="./?ch=5">Continue reading →</a></p>
</section>
<section id="ch-6">
<h2><a href="./?ch=6">Chapter 5: Splitting the System Without Splitting the Problem</a></h2>
<p>A logistics company needs to add a new delivery status, “held at customs,” to the tracking flow visible to customers. The status already exists in the customs clearance system. It needs to propagate to the shipment tracking service, which feeds the customer-facing portal.</p>
<p>In a system with real boundaries and owned contracts, this is two days of work. One team adds the status to its published event contract. The consuming team reads the new event and renders it in the portal. Both teams can do this independently, on their own release schedules, with a versioned contract governing the interaction.</p>
<p>In this organisation, none of that applies.</p>
<p class="continue"><a href="./?ch=6">Continue reading →</a></p>
</section>
<h2 class="part">Capital Mispricing and Structural Risk</h2>
<section id="ch-7">
<h2><a href="./?ch=7">Chapter 6: Lag Governance and Capital Mispricing</a></h2>
<p>A European insurer with approximately five hundred services invests € 1.2 million in a shared integration layer for its claims process: a common event infrastructure that would let the claims adjudication, policy, and payments teams exchange information independently, replacing a fragile web of direct connections between services that causes an average of three incidents per month.</p>
<p>The platform team delivers a working prototype in four months. It is technically sound. The three consuming teams are willing to migrate. The projected incident reduction is 70--80\</p>
<p>The CFO kills the investment in the annual budget review. The platform “does not show ROI in year one.” The cost is visible: € 1.2 million in engineering salaries and infrastructure. The benefit is diffuse: fewer incidents, faster change, reduced coordination overhead. None of these benefits appear as a line item in any business unit’s P\&amp;L.</p>
<p class="continue"><a href="./?ch=7">Continue reading →</a></p>
</section>
<section id="ch-8">
<h2><a href="./?ch=8">Chapter 7: The Coordination Tax and the Alignment Supply Chain</a></h2>
<p>The delivery lead’s calendar shows forty-one hours of meetings this week. He has blocked two hours on Thursday morning for “focus time.” By Tuesday afternoon, a programme steering committee has been scheduled over it.</p>
<p>He does not build anything. He does not decide anything. He translates. He takes what the product manager says the claims process needs, what the architect says the system can support, and what the engineering teams say they can deliver, and he reconciles them into a plan that satisfies all three. The plan is presented weekly. It is revised weekly. It has never once survived contact with the sprint.</p>
<p>His role exists because no one in the organisation can talk directly to anyone else. Product cannot talk to engineering because they do not share a vocabulary. Architecture cannot talk to delivery because their artefacts describe different systems. Engineering cannot talk to the business because the business does not attend their meetings. He is the connective tissue.</p>
<p>He is very good at his job. He is also exhausted, because his job would not exist if the organisation were willing to put the people who decide and the people who build in the same room with shared accountability.</p>
<p>These coping mechanisms were not accidental survivors. They were institutionalised, monetised, and scaled into a system of roles, frameworks, incentives, and vendors whose continued operation requires the gap between narrative and system to persist.</p>
<p>This system is the Alignment-Industrial Complex.</p>
<p>The delivery lead in the vignette is one node in an extensive supply chain of translation. Programme managers reconcile timelines. Delivery leads reconcile product intent with engineering capacity. Portfolio analysts reconcile investment with outcomes across a taxonomy that does not match the system. Each role exists because two parts of the organisation that should be connected are not. Remove the role, and the contradiction surfaces, which is why the role persists.</p>
<p>In a typical mid-sized software-dependent corporate, 30--40\</p>
<p>The most durable feature of the alignment supply chain is that it insulates senior leadership from early signal.</p>
<p>Dashboards lag reality. Reports summarise interpretation. Status is curated. Each layer absorbs contradiction before it travels upward. By the time a problem reaches the board, it is already framed as a delivery issue, a talent issue, or a market shift. The structural cause has been translated out of existence.</p>
<p>No one inside the system is incentivised to surface accurate signal early. Doing so creates friction without authority. External advisors face the same constraint: their continued engagement depends on recommendations that operate within the existing power structure. The result is that signal arrives late, after the window for inexpensive correction has closed.</p>
<p>The insulation extends to the delivery frameworks themselves. Many were created inside organisations that had already done the structural work. When imported into environments where ownership is diffuse, they invert their purpose: autonomy becomes symbolic, authority remains centralised, and coordination overhead increases but is now described as maturity.</p>
<p>A board approves a € 400,000 diagnostic engagement. The brief is familiar: technology investment is not producing proportional outcomes. The CEO suspects something structural but cannot name it.</p>
<p>The consultants conduct thirty interviews in three weeks. They are thorough. They map dependencies, interview engineers, and build a picture that is, in its essentials, accurate: ownership is fragmented, strategy is disconnected from system behaviour, and coordination has replaced autonomy. The findings are presented to the engagement partner. She does not dispute the diagnosis. She explains which parts of it the client can absorb.</p>
<p>The recommendation that reaches the board is 120 slides. It is well-structured and internally coherent. It proposes a new operating model with clearer accountabilities, a governance framework that connects strategy to delivery, and an eighteen-month implementation programme. The follow-on engagement is approved at the next board meeting. The board does not know what the original findings said. It does not know that the diagnosis named the organisational structure as the cause, or that this finding was replaced with a recommendation for better alignment. The board approved the budget for the diagnostic. It received the deliverable. The space between the two, where the diagnosis was softened into a prescription the client could absorb, is invisible from the boardroom.</p>
<p>Fourteen months later, the organisation has more governance forums, more coordination roles, and a transformation programme that is “on track.” The structural problem persists. The next diagnostic will find the same thing.</p>
<p>Consulting firms are hired to resolve misalignment, and their recommendations must be acceptable to the executives who commissioned the engagement. Their diagnoses therefore converge on more alignment, more governance, more structure layered on top of existing structure.</p>
<p>The incentive is easier to see outside software. Commission an engineer to inspect a building whose tenants must not be disturbed. She identifies subsidence in the east wing and recommends underpinning. Underpinning requires evacuating the east wing. The tenants must not be disturbed. She revises the recommendation to monitoring. The monitoring confirms further movement. She recommends a follow-up survey. The building is subsiding, the engineer has confirmed it in writing, and the deliverable is a schedule for watching it subside. The invoice is for structural engineering, which is what most consulting engagements deliver: a professional record of watching the problem continue.</p>
<p>Clarity that removes the need for external coordination is not a deliverable the engagement model supports. The complex therefore optimises, rationally, for outcomes that sustain the conditions requiring alignment. This is not a conspiracy but a market responding to incentive.</p>
<p>Alignment survives because it is the last safe language: one that acknowledges problems without naming causes, that responds to dysfunction without redistributing power.</p>
<p>AI breaks this equilibrium, because when synthesis becomes cheap, alignment becomes expensive. When artefacts can be read directly, translation layers lose value, and when behaviour can be inspected continuously, consensus without ownership is exposed.</p>
<p>The coordination tax is not an operational inefficiency. It is the cost of refusing to close the gap between narrative and system. AI does not eliminate the tax. It makes the tax visible, enumerable, and attributable, which is the precondition for deciding whether to continue paying it.</p>
</section>
<section id="ch-9">
<h2><a href="./?ch=9">Chapter 8: Talent Decay as Structural Fragility</a></h2>
<p>A principal engineer at a European insurer with approximately five hundred services and a claims management platform that has been accumulating complexity for fifteen years raises a concern about a dependency that will block the roadmap. The concern is noted but not addressed. She flags a risk in an architecture review. The risk is acknowledged but deprioritised. She points out that a commitment made to the board cannot be delivered as described. She is told to find a way.</p>
<p>After enough of these exchanges, she stops. Not because she has lost the knowledge. She still sees the problems clearly, still traces the dependencies in her head, still knows which parts of the system are fragile and why. The cost of stating what she sees has become higher than the cost of staying silent. The silence is not apathy. It is the particular exhaustion of someone who has been right, repeatedly, to no effect.</p>
<p>Two years later, she leaves. She writes a resignation letter that takes ten minutes, having mentally composed it for six months. She takes with her an understanding of the claims adjudication process, the policy integration constraints, the payment reconciliation edge cases, and the failure modes of the document management service that no document in the organisation captures. Her replacement arrives with equivalent credentials and no context. He spends six months learning what she knew on her first day. Some of what she knew, he never learns, because it was never written down.</p>
<p class="continue"><a href="./?ch=9">Continue reading →</a></p>
</section>
<section id="ch-10">
<h2><a href="./?ch=10">Chapter 9: Operational and Regulatory Exposure</a></h2>
<p>A European insurer receives notice of a regulatory change: all policies above € 25,000 annual premium must observe a mandatory 48-hour cooling-off period before activation. During this window, payment may be collected, the policy may be cancelled without penalty, claims are not valid, and risk exposure must not be recognised on the balance sheet.</p>
<p>This is a change to the policy lifecycle, not a feature request.</p>
<p>The insurer has five hundred services in its technology estate. “Activation” is not one system. It spans five: quoting, payments, the policy record, claims eligibility, and broker commissions. Each of these feeds data to dozens of other systems. The policy record alone is consumed by fifty-seven.</p>
<p class="continue"><a href="./?ch=10">Continue reading →</a></p>
</section>
<section id="ch-11">
<h2><a href="./?ch=11">Chapter 10: Who Loses When Clarity Wins</a></h2>
<p>A programme director at a European insurer watches the first autonomous unit’s quarterly results. Claims resolution time has dropped 31\</p>
<p>He reads the unit’s results carefully. He is not hostile to the model. He can see that it works. He can also see that his role exists because the organisation has not done what the unit demonstrates is possible. His programme coordinates across teams that do not share ownership. The unit owns its process and coordinates through contracts. His programme produces status reports. The unit produces outcomes.</p>
<p>He begins drafting a proposal for a governance review of the unit’s deployment cadence.</p>
<p class="continue"><a href="./?ch=11">Continue reading →</a></p>
</section>
<h2 class="part">Structural Correction</h2>
<section id="ch-12">
<h2><a href="./?ch=12">Chapter 11: The Unit That Owns the Truth</a></h2>
<p>The payments unit has nine people. Two senior engineers, three engineers, a domain expert named Marta who spent twelve years in payments operations before joining the unit, a data analyst, a reliability engineer, and a unit lead who was previously a senior architect.</p>
<p>They own the payments process: premium collection, claims disbursement, broker commission payments, and refunds. They own the services that implement these flows, the data those services produce, and the contracts through which other units interact with them.</p>
<p>On a Wednesday morning, Marta notices that the refund process has a failure rate that has crept from 0.3\</p>
<p class="continue"><a href="./?ch=12">Continue reading →</a></p>
</section>
<section id="ch-13">
<h2><a href="./?ch=13">Chapter 12: Contracts, Not Committees</a></h2>
<p>The payments unit’s contract requires consuming units to submit payment requests with fully validated customer and account data. This is structurally correct: the payments process should not validate data it does not own.</p>
<p>The consuming units experience this differently. Motor origination finds that validating account data at point of sale adds delay that customers feel. Claims discovers that claimants who have changed banks require a re-verification step that its process definition does not accommodate. Both units face the same incentive: minimise their own validation costs and let the payments unit reject what it cannot process.</p>
<p>The payments unit tightens its contract: stricter formats, more required fields, faster rejection of anything that does not comply. The consuming units add workarounds and automatic retries to handle the rejections. Within six months, the interaction has produced coordination overhead encoded in contracts rather than in meetings, but coordination overhead nonetheless. Payment failures increase. Resolution times lengthen.</p>
<p class="continue"><a href="./?ch=13">Continue reading →</a></p>
</section>
<section id="ch-14">
<h2><a href="./?ch=14">Chapter 13: Data as Outputs, Not a Shared Substrate</a></h2>
<p>A data analyst in the central reporting team at a Nordic insurer spent three weeks building the quarterly capital allocation report for the CFO. The report required revenue-per-customer figures across three business lines: motor, property, and commercial. He pulled the numbers from each division’s data warehouse. They did not reconcile.</p>
<p>Motor counted a customer as a policyholder. Property counted a customer as a household. Commercial counted a customer as a legal entity with an active contract in the trailing twelve months. A single corporate client with a fleet policy, a building policy, and a liability policy appeared as three customers in one view, one customer in another, and either one or zero in the third, depending on the contract renewal date.</p>
<p>He spent four days in meetings with divisional data owners, trying to agree a common definition. They could not. Each definition was correct within its own process. He built a reconciliation layer: a spreadsheet that mapped between the three definitions using manual rules he documented in a tab labelled “assumptions.” The tab ran to forty rows.</p>
<p class="continue"><a href="./?ch=14">Continue reading →</a></p>
</section>
<section id="ch-15">
<h2><a href="./?ch=15">Chapter 14: What to Demand Before Anything Else</a></h2>
<p>The audit committee met quarterly, but this session had an additional agenda item. The chair, a non-executive director who had spent thirty years in operational finance before joining the board, had read the AI readiness report that management had circulated the previous week. Fourteen pages. She had three questions written on a folded sheet of paper in front of her.</p>
<p>The CTO was present, which was unusual. The CFO sat beside him. The chief risk officer attended by video from Singapore. The committee secretary had reserved ninety minutes. The chair suspected they would need less.</p>
<p>She opened her folded sheet and read the first question.</p>
<p class="continue"><a href="./?ch=15">Continue reading →</a></p>
</section>
<section id="ch-16">
<h2><a href="./?ch=16">Chapter 15: The Transition and Its Failure Modes</a></h2>
<p>Each executive role is affected differently by the structural corrections described in the preceding chapters. What follows describes, for each, what stops being governable by narrative, what new artefacts become decisive, what to stop rewarding, and what to authorise. First: the structural shift that applies to all of them, and the transition itself.</p>
<p>Most executives reached their position by operating at distance from mechanism: synthesising across functions, weighing trade-offs, allocating capital without holding every operational detail. This distance is how leadership scales. Over time, it becomes identity.</p>
<p>When processes become explicit and contracts become binding, this pattern changes. Machine-readable artefacts shorten the path between commitment and consequence. The executive who is comfortable experiences it as increased control. The executive who is not experiences it as the loss of interpretive insulation: the capacity to reframe outcomes, diffuse responsibility, and manage contradiction through narrative rather than mechanism. Boards are not always the victims of this resistance. In some organisations, the board is a co-participant: preferring the smoothed narrative because interrogating it would require confronting structural problems the board itself has tolerated.</p>
<p class="continue"><a href="./?ch=16">Continue reading →</a></p>
</section>
<section id="ch-17">
<h2><a href="./?ch=17">Chapter 16: What the Sceptical Board Member Asks</a></h2>
<p>The offsite is held at a hotel outside Munich. The CEO has spent forty minutes presenting the structural correction programme: autonomous units, process ownership, versioned contracts, continuous reconciliation. The slides are uncharacteristically specific. The CFO has presented the envelope budgeting model. The CTO has described the first unit’s charter and its ninety-day results.</p>
<p>A non-executive director, twelve years on the board, former COO of a listed logistics company, has been listening without interruption. She opens her notebook. The CEO recognises the gesture. It means questions are coming.</p>
<p>The objections that follow are predictable. They are also rational within the current structure, which is precisely the point.</p>
<p class="continue"><a href="./?ch=17">Continue reading →</a></p>
</section>
<section id="ch-18">
<h2><a href="./?ch=18">Conclusion</a></h2>
<p>\addtocontentstoc\protect\addvspace0.5em</p>
<p>A board member opens a reconciliation report at 7:40 on a Tuesday morning, twenty minutes before the quarterly review. The report is six pages. It compares the renewals process definition against the code that implements it. She reads that the automated risk reassessment described in the strategy update has not been implemented. The renewals process still routes every modified policy to a manual review queue. The queue is staffed by four people. Average time in queue: eleven days. The strategy update she approved last quarter described the reassessment as “deployed and in optimisation.” She turns to the management narrative prepared for this morning’s meeting. Page three describes renewals performance as “on track, with automated reassessment reducing processing time by 40\</p>
<p>She sets the management narrative down. She does not speak. She does not need to.</p>
<p>This book has described a structural problem and a structural solution. The problem is that most large organisations cannot describe themselves clearly enough for AI to produce anything other than faster narrative. The solution is to make the organisation legible: explicit processes, owned boundaries, versioned contracts, structured artefacts that can be tested against the systems that run the business.</p>
<p>Chapter 2 described the fork: AI as a writer of narrative, or AI as a reader of systems. The two are contradictory. Without the structural changes described in Part IV, AI reads whatever the organisation gives it and the organisation concludes that synthesis “does not work yet.” The retreat to the writing path is not a technology failure. It is a structural one.</p>
<p>The two paths diverge, and the divergence compounds. Each quarter that an organisation operates AI as a narrative tool, generating board packs, polishing status reports, producing strategy updates, builds institutional dependence on the output. Roles form around producing and approving it. The board adjusts to receiving it. Switching to the reader path then requires not merely deploying a different capability but withdrawing confidence from the narrative infrastructure the organisation has spent years constructing. The political cost of that switch rises with every governance cycle. Deferral does not preserve the option. It narrows it.</p>
<p>The forces described in Part III reinforce this narrowing. Regulatory frameworks already require traceable process evidence that narrative governance cannot produce cheaply. Deferred platform investments accumulate incident costs that reconciliation can now connect to their structural cause. The experienced engineers who could make processes explicit are leaving, and each departure destroys context that was never captured in any artefact. These pressures do not arrive as a single crisis. They arrive as a narrowing window in which the cost of the structural work rises while the organisational capacity to do it declines.</p>
<p>Once the board has seen a reconciled synthesis, the position shifts from ignorance to informed inaction. The former is defensible. The latter is a governance risk that compounds with every board cycle in which the findings are noted and not acted on.</p>
<p>The structural changes cannot be delegated to a transformation programme, because transformation programmes are approved by the same executives whose authority the change redistributes. They cannot be delegated to the technology team, because the change is organisational. They cannot be delegated to a consultancy, because consultancies must produce recommendations acceptable to the executives who commissioned the engagement. The choice must be made by the people with authority to redistribute power: the CEO and the board.</p>
<p>Take one critical business process: claims adjudication, mortgage origination, payment settlement. Ask: does a machine-readable description exist? Can it be reconciled against the code? Can the reconciliation identify contradictions without human interpretation? If the answer to any of these is no, the organisation has identified the first unit of structural work. Every AI investment it makes until that work is done will produce faster narrative rather than structural improvement, and every narrative it produces becomes a discoverable artefact that reconciliation can test.</p>
<p>Once reconciliation is technically possible, the absence of reconciliation becomes evidence. Reconciliation outputs are records. In any subsequent regulatory inquiry, acquisition due diligence, or internal audit, the question is no longer whether the organisation could have verified its own representations. The capability exists at negligible cost. The question becomes why it was not used, and the board’s answer is now part of the governance record.</p>
<p>The capacity for structural honesty now exists. It does not require new technology, a transformation programme, or external validation. It requires a decision by the people with authority over the organisation’s structure. The structural preconditions take years to build and cannot be purchased when the need becomes urgent. I have worked inside organisations on both sides of this choice. The ones that build legibility first discover that every subsequent demand, regulatory, operational, strategic, is cheaper because the organisation can already describe itself. The ones that defer discover the opposite. The gap between what an organisation says and what it does is now measurable at negligible cost, and that measurement does not wait for the board to commission it.</p>
</section>
<section id="ch-19">
<h2><a href="./?ch=19">About the Author</a></h2>
<p>Adrian McPhee has spent more than twenty years as an engineer, architect, and CTO inside software-dependent corporates across fintech, retail, and mobility, including a unicorn fintech, a leading ecommerce marketplace, and a multinational with €10 billion turnover. He has led organisations of up to a thousand people, delivered global platforms, and redirected more than €200M of product and technology investment. He watched the same structural dysfunctions repeat across every organisation he entered, and eventually learned to dismantle them.</p>
<p>As a founder and entrepreneur, he also designs and delivers platforms from scratch. It is the same discipline this book argues for: explicit structure, reconciled with reality, that a machine can actually read.</p>
<p>He now works with leadership teams to diagnose and dismantle the patterns described in this book. The operational perspective on these patterns, how processes become implicit, how architecture becomes avoidant, and what changes when they are made explicit, is examined in Illusions of Work, which is written for the CTO and engineering leadership audience that must execute the structural changes this book authorises.</p>
<p class="continue"><a href="./?ch=19">Continue reading →</a></p>
</section>

  <div class="cta">
    <p>Read the full book in the interactive reader</p>
    <a href="./">Start reading</a>
  </div>

  <footer>
    <p>&copy; Adrian McPhee 2026 &middot;
    <a href="https://www.linkedin.com/in/adrianmcphee/" style="color: #999;">Contact</a></p>
  </footer>
</body>
</html>